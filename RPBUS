---
title: "MILESTONE-REPORT"
author: "SMS"
date: "8 de maio de 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## THE PROJECT
The purpose of the current project is to create a prediction algorithm, which can be used in a Shiny app, having as a starting point and data contained in the Coursera-SwiftKey file. This file consists of quotations and sentences from blogs, news and twitter in four languages: German, Finnish, Russian and English. However, as I do not know the other languages, I only worked with the data in English.

As for the creation of an algorithm for the prediction of words and phrases contained in blogs, news and twitter, I chose for an exploratory analysis the following packages:

```{r packages}
setwd("C:/Users/smsanda/Documents/en_US")

library(tm) #A framework for text mining applications within R.
library(RWeka) #Weka is a collection of machine learning algorithms for data mining tasks written in Java, containing tools for data pre-processing, classification, regression, clustering, association rules, and visualization. Package 'RWeka' contains the interface code, the Weka jar is in a separate package 'RWekajars'
library(wordcloud) #can be an handy tool when you need to highlight the most commonly cited words in a text using a quick visualization
library(stringi) #Allows for fast, correct, consistent, portable, as well as convenient character string/text processing in every locale and any native encoding
library(SnowballC) #An R interface to the C libstemmer library that implements Porter's word stemming algorithm for collapsing words to a common root to aid comparison of vocabulary
```

##READ THE FILES AND GET MORE INFORMATION:
```{r data}
news <- readLines(file("en_US.news.txt"))
head(news)
blogs <- readLines(file("en_US.blogs.txt"))
head(blogs)
twitter<- readLines(file("en_US.twitter.txt"))
head(twitter)

# File sizes
blogs.size <- file.info("en_US.blogs.txt")$size / 1024 ^ 2
news.size <- file.info("en_US.news.txt")$size / 1024 ^ 2
twitter.size <- file.info("en_US.twitter.txt")$size / 1024 ^ 2

# Accounting the words in files
blogs.words <- stri_count_words(blogs)
news.words <- stri_count_words(news)
twitter.words <- stri_count_words(twitter)

# Summary the data
summary(stri_count_words(blogs))
summary(stri_count_words(news))
summary(stri_count_words(twitter))
```
# PREPARE THE DATA FRAME
```{r data frame}
data.frame(source = c("blogs", "news", "twitter"),         
           file.size.MB = c(blogs.size, news.size, twitter.size),
           num.lines = c(length(blogs), length(news), length(twitter)),
           num.words = c(sum(blogs.words), sum(news.words), sum(twitter.words)),
           mean.num.words = c(mean(blogs.words), mean(news.words), mean(twitter.words)))
```
# CREATE CORPUS
Merge the data frames by common columns or row names for Load 5000 lines from every set in corpus
```{r corpus}
merged <- paste(news[1:5000], blogs[1:5000], twitter[1:5000])
corpus <- VCorpus(VectorSource(merged))
```
# REMOVE LARGE FILE TO CLEAN UP THE MEMORY
To prevent memory from being clogged by intermediate files use the rm command
```{r save}
rm (blogs.words)
rm(news.words)
rm(twitter.words)

rm(blogs)
rm(news)
rm(twitter)
```
## CLEAN DATA USING TM PACKAGE
Remove extra white space, numbers, punctuation, etc.
```{r clean}
library(tm)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords())
corpus <- tm_map(corpus, stemDocument)
```
#TOKENIZE (n-Gram Creation)
Use the RWeka package to create different n-grams of the sample corpus, but first you will have to separate the sentence once the target is to predict the next word in a sentence.
```{r token}
library(RWeka)
corpusDf <-data.frame(text=unlist(sapply(corpus, 
                                         `[`, "content")), stringsAsFactors=F)

findNGrams <- function(corp, grams) {
ngram <- NGramTokenizer(corp, Weka_control(min = grams, max = grams,
                                             delimiters = " \\r\\n\\t.,;:\"()?!"))
ngrama <- data.frame(table(ngram))
ngramb <- ngrama[order(ngrama$Freq,decreasing = TRUE),][1:100,]
colnames(ngramb) <- c("String","Count")
ngramb
}

#1-4 grams  tokenize
UniGrams <- findNGrams(corpusDf,1)
TwoGrams <- findNGrams(corpusDf,2)
ThreeGrams <- findNGrams(corpusDf,3)
FourGrams <- findNGrams(corpusDf,4)
```

##WORD CLOUDS AND HISTOGRAMS

The most frequent words in each gram and the prove that cleaning processes worked.

```{r wordcloud, echo=FALSE}
library(wordcloud)
## Loading required package: RColorBrewer
require(RColorBrewer)

par(mfrow = c(1, 3))
palette <- brewer.pal(8,"Dark2")

wordcloud(UniGrams[,1], UniGrams[,2], min.freq =1, 
          random.order = F, ordered.colors = F, colors=palette)
text(x=0.5, y=0, "1-gram cloud")

wordcloud(TwoGrams[,1], TwoGrams[,2], min.freq =1, 
random.order = F, ordered.colors = F, colors=palette) 
text(x=0.5, y=0, "2-gram cloud")

wordcloud(ThreeGrams[,1], ThreeGrams[,2], min.freq =1, 
          random.order = F, ordered.colors = F, colors=palette)
text(x=0.5, y=0, "3-gram cloud")

wordcloud(FourGrams[,1], FourGrams[,2], min.freq =1, 
          random.order = F, ordered.colors = F, colors=palette)
text(x=0.5, y=0, "4-gram cloud")
```

#HISTOGRAMS
```{r histogram, echo=FALSE}
par(mfrow = c(1, 1))

barplot(TwoGrams[1:20,2], cex.names=0.5, names.arg=UniGrams[1:20,1], col="blue", main="Top 20 most common in sample 1-Grams", las=2)

barplot(TwoGrams[1:20,2], cex.names=0.5, names.arg=TwoGrams[1:20,1], col="purple", main="Top 20 most common in sample 2-Grams", las=2)

barplot(ThreeGrams[1:20,2], cex.names=0.5, names.arg=ThreeGrams[1:20,1], col="gray", main="Top 20 most common in sample 3-Grams", las=2)

barplot(FourGrams[1:20,2], cex.names=0.5, names.arg=FourGrams[1:20,1], col="black", main="Top 20 most common in sample 4-Grams", las=2)
```

#CONCLUSION
In a first analysis, the creation of the algorithm of prediction and use in a ShinyApp application should occur through the use of the n-gram model to predict the next word. For example, if the algorithm does not find any matching quadrigram, then the algorithm would go back to the tigram model and then to the bigram model until it reaches the unigram.
However, I expect a certain delay in processing due to the size of the files.

Regarding the Shiny App I intend to create an interface for the user, which will contain a text input box in which he will enter a phrase so that the algarÃ­timo informs the probable words following the sentence
