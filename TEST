setwd("C:/Users/smsanda/Documents/en_US")

library(tm) #A framework for text mining applications within R.
library(RWeka) #Weka is a collection of machine learning algorithms for data mining tasks written in Java, containing tools for data pre-processing, classification, regression, clustering, association rules, and visualization. Package 'RWeka' contains the interface code, the Weka jar is in a separate package 'RWekajars'
library(wordcloud) #can be an handy tool when you need to highlight the most commonly cited words in a text using a quick visualization
library(stringi) #Allows for fast, correct, consistent, portable, as well as convenient character string/text processing in every locale and any native encoding
library(SnowballC) #An R interface to the C libstemmer library that implements Porter's word stemming algorithm for collapsing words to a common root to aid comparison of vocabulary


news <- readLines(file("en_US.news.txt"))
head(news)
blogs <- readLines(file("en_US.blogs.txt"))
head(blogs)
twitter<- readLines(file("en_US.twitter.txt"))
head(twitter)

# Get file sizes
blogs.size <- file.info("en_US.blogs.txt")$size / 1024 ^ 2
news.size <- file.info("en_US.news.txt")$size / 1024 ^ 2
twitter.size <- file.info("en_US.twitter.txt")$size / 1024 ^ 2

# Get words in files
blogs.words <- stri_count_words(blogs)
news.words <- stri_count_words(news)
twitter.words <- stri_count_words(twitter)

# Summary the data
summary(stri_count_words(blogs))
summary(stri_count_words(news))
summary(stri_count_words(twitter))

# Prepare the data.frame
data.frame(source = c("blogs", "news", "twitter"),         
           file.size.MB = c(blogs.size, news.size, twitter.size),
           num.lines = c(length(blogs), length(news), length(twitter)),
           num.words = c(sum(blogs.words), sum(news.words), sum(twitter.words)),
           mean.num.words = c(mean(blogs.words), mean(news.words), mean(twitter.words)))
               
# Create corpus
# Merge the data frames by common columns or row names
# Load 5000 lines from every set in corpus
merged <- paste(news[1:5000], blogs[1:5000], twitter[1:5000])
corpus <- VCorpus(VectorSource(merged))

# Remove large files to clean up memory
rm (blogs.words)
rm(news.words)
rm(twitter.words)

rm(blogs)
rm(news)
rm(twitter)

## CLEAN DATA USING TM PACKAGE
#Remove extra white space, numbers, punctuation, etc.
library(tm)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords())
corpus <- tm_map(corpus, stemDocument)

#TOKENIZE (n-Gram Creation)
# Use the RWeka package to create different n-grams of the sample corpus, but first you will have to separate the sentence once the target is to predict the next word in a sentence.

library(RWeka)
corpusDf <-data.frame(text=unlist(sapply(corpus, 
                                         `[`, "content")), stringsAsFactors=F)

findNGrams <- function(corp, grams) {
ngram <- NGramTokenizer(corp, Weka_control(min = grams, max = grams,
                                             delimiters = " \\r\\n\\t.,;:\"()?!"))
ngrama <- data.frame(table(ngram))
ngramb <- ngrama[order(ngrama$Freq,decreasing = TRUE),][1:100,]
colnames(ngramb) <- c("String","Count")
ngramb
}

#1-4 grams  tokenize
UniGrams <- findNGrams(corpusDf,1)
TwoGrams <- findNGrams(corpusDf,2)
ThreeGrams <- findNGrams(corpusDf,3)
FourGrams <- findNGrams(corpusDf,4)

#Plot word clouds and histograms
#The most frequent words in each gram and the prove that cleaning processes worked.

library(wordcloud)
## Loading required package: RColorBrewer
require(RColorBrewer)

par(mfrow = c(1, 3))
palette <- brewer.pal(8,"Dark2")

wordcloud(UniGrams[,1], UniGrams[,2], min.freq =1, 
          random.order = F, ordered.colors = F, colors=palette)
text(x=0.5, y=0, "1-gram cloud")

wordcloud(TwoGrams[,1], TwoGrams[,2], min.freq =1, 
random.order = F, ordered.colors = F, colors=palette) 
text(x=0.5, y=0, "2-gram cloud")

wordcloud(ThreeGrams[,1], ThreeGrams[,2], min.freq =1, 
          random.order = F, ordered.colors = F, colors=palette)
text(x=0.5, y=0, "3-gram cloud")

wordcloud(FourGrams[,1], FourGrams[,2], min.freq =1, 
          random.order = F, ordered.colors = F, colors=palette)
text(x=0.5, y=0, "4-gram cloud")

par(mfrow = c(1, 1))

barplot(TwoGrams[1:20,2], cex.names=0.5, names.arg=UniGrams[1:20,1], col="blue", main="Top 20 most common in sample 1-Grams", las=2)

barplot(TwoGrams[1:20,2], cex.names=0.5, names.arg=TwoGrams[1:20,1], col="purple", main="Top 20 most common in sample 2-Grams", las=2)

barplot(ThreeGrams[1:20,2], cex.names=0.5, names.arg=ThreeGrams[1:20,1], col="gray", main="Top 20 most common in sample 3-Grams", las=2)

barplot(FourGrams[1:20,2], cex.names=0.5, names.arg=FourGrams[1:20,1], col="black", main="Top 20 most common in sample 4-Grams", las=2)

#In the milestone report, it allows me to have a better idea about the dataset we will be dealing with. However for the final project - creating a predictive word ShinyApp will be a totally different attempt.Despite limited the dataset to 150,000, the amount of time needed to run the codes is still considerable long. Hence I will expect even more time and resources are needed for doing up the Shinyapp.
